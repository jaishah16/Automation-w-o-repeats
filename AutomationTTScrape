from selenium import webdriver
import time
from scrapy import Selector
import json
import csv
import requests
from datetime import datetime, timezone

import schedule

###
PHASE 1 HERE
###
def phase_1():
    SCROLL_PAUSE_TIME = 15  
    driver = webdriver.Firefox()

    def get_profile_data(url):
        driver.get(url)
        time.sleep(SCROLL_PAUSE_TIME)

        videos_list = None

        str = "const origOpen=XMLHttpRequest.prototype.open;XMLHttpRequest.prototype.open=function(t,e){this.addEventListener(\"load\",function(){var t;4===this.readyState&&isVideoFetch(e)&&(pushVideoIDs(t=JSON.parse(this.responseText)),checkAutoScroller(t))}),origOpen.apply(this,arguments)};const autoScroller=setInterval(function(){window.scrollTo(0,document.body.scrollHeight)},1e3);function isVideoFetch(t){return/\/api\/post\/item_list\//.test(t)}function pushVideoIDs(t){t.itemList.forEach(t=>{})}function checkAutoScroller(t){t.hasMore||clearInterval(autoScroller)}"

        time.sleep(5)
        driver.execute_script(str)
        time.sleep(SCROLL_PAUSE_TIME)
        response = Selector(text=driver.page_source)

        videos_list = response.css('[data-e2e="user-post-item-desc"] a::attr(href)').getall()
        videos_list = [url for url in videos_list if 'https' in url]

        title = response.css('[data-e2e="user-title"] ::text').get()
        sub_title = response.css('[data-e2e="user-subtitle"] ::text').get()
        following = response.css('[data-e2e="following-count"] ::text').get()
        followers = response.css('[data-e2e="followers-count"] ::text').get()
        likes = response.css('[data-e2e="likes-count"] ::text').get()
        user_bio = response.css('[data-e2e="user-bio"] ::text').get()
        verified = "verified" if bool(response.css('[data-e2e="user-title"] svg')) else "not verified"
        link = response.css('span.css-847r2g-SpanLink.eht0fek2 ::text').get()

        return {
            "title": title,
            "url": url,
            "sub_title": sub_title,
            "following": following,
            "verified": verified,
            "followers": followers,
            "likes": likes,
            "user_bio": user_bio,
            "link": link,
            "videos_list": videos_list,
            "total_videos": len(videos_list),
        }

    profile_urls = []
    with open('input.csv', 'r', newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            url = row['URLs']
            profile_urls.append(url)

    main_data = []

    for index, profile_url in enumerate(profile_urls):
        profile_data = get_profile_data(profile_url)
        main_data.append(profile_data)
        with open('output.json', 'w') as json_file: 
            json.dump(main_data, json_file)
        print(index)

###
PHASE 1 HERE
###
def phase_2():
    # Define cookies and headers
    cookies = {
        'tt_chain_token': 'p4Y3pQQXlljXj/aNR92T0g==',
        'tiktok_webapp_theme': 'light',
        'odin_tt': 'af597dadca65f5e25cd5e17851d7afc592783c7fcecb928a38162970029173effb143cec4ebab8425585a9d262165080a2ce77881b8297dbd274b572d63701b807277e79338f6e538fc870a3c331227f',
        'ttwid': '1%7Cit6LEsZ0QnYg2nzXdcKk7BP7GW_BhjZdFNN3LYcc99s%7C1702645029%7C11c20e2b073f7f775cec26373104f5e1e5b21691bfa46cccc3e7eb58ecb60d1c',
        'msToken': 'oWz6-lEjBLOSWov77V03dZLCt7LL_kw4_pElqWdKzYW936UnwSBY8NUc95lMEZ5vMWRUFvKUIxQ-ZiDzxefXsLjt4V2OZPy94CCKyHvQkmunYrHxwbM5fDRlGWIUg3XE5PE-PdU=',
        'tt_csrf_token': 'pAV3BIzP-b9zBu0PygCC8cac51riivcmHPLE',
        'passport_csrf_token': '69dda2cf0e38e30143c5fbd7d00caff2',
        'passport_csrf_token_default': '69dda2cf0e38e30143c5fbd7d00caff2',
        'bm_mi': '02BE7129E3879197BD74C9FABF35DDBF~YAAQFpqwtoOkX16NAQAAdUHlXxYxegeFeCpr3TBmq9wVDx4s/bZCmumFakOM48pYcy+u6OKQ7oXuWpWiaTf3tzykD6sA9AOzHSMKwmpcxY5KWjsL5jq5RMOWqRPepKSgvCU6cSgNN+1A+/lsKZL1dWVYaVfLHD1rQCVrsfEkydX4EfMD9zrYb//f9bqjUFf8Fry9RupVGZdnlg27xJef3GuIPlxU9dPoqpyDy0p7DiWzeNVePxuYxhNtIMjTWm1pJ+eYyth5boq7B/bKmspOGdu+rawU5SJB5f+jqqBy7WIH3bWtWkPqyO+BIBcD~1',
        'bm_sv': 'F498AF2966B0AD5DDFB18ECA730026C5~YAAQFpqwtoSkX16NAQAAdUHlXxZJjOqqEKFlhgfEs1+mnPu9RLWIdw53fin/Ehz8itmBSAlroV0w6fK/kufrPp1YjqcniVJS4jAn4InjN3qoXFzYAMPiOL0AYB//UanjTbCF9NINNScct+tIV7uPnNaIcG52c0KApIrOiaxijjAzwsYu5Qw7M4ZIu9srX7FSldTA1yk52pIoQDfxOTN/4NBkZuVqZjM6TMhbal2fetbOw+HsNVDW5jwHvgPMvKk5~1',
        'ak_bmsc': '4959182BFEA6D1E6FE12E8EDD3F30004~000000000000000000000000000000~YAAQFpqwtpKkX16NAQAA0YblXxbuGFuDB2IguaXXCq1CfPdUEeecXQV4cvhheEfpmKvQ9QeB9VL4ham69THBRsZHJY7aZuIs2EHDSwm7OPsy+2Mmgx31wD14C5S1/Y679djTZQWf7N3ubiobnp9yzQQwrg3ZJykRZs4w59vUiOLAV9NM3jVzZ1MGlcAdDzEZSiKYGkdhrChliKVk5LY9mvrrcUMG33PxOGl7SY6CQwiJtkxBg00u+qWfxlqNJxX3TxznAgXb5QOd6ZeQTsUVktkkYeoMzIzrrLh9dsuHfxuaatwkOfezgY6e+34kV7ZQPF4bvBhxvNzox63tYn51vS2H0H6KCjQOkmGk0DVvCpx1tKS0HcC9qDiZpqZ1qnebKns+9O313NUwLOFYra97DsfNDd3N76j6ezC2AOs=',
        'perf_feed_cache': '{%22expireTimestamp%22:1706882400000%2C%22itemIds%22:[%227322960513669811488%22%2C%227308072540189003041%22]}',
        'msToken': 'zFsyrJ6m7a3XoH6CQHU0GaYrRkXLog9TGfIcNLpk4EyUYxKAbwYkuSzCjF6dlVs9mJr0z7HcHGbGkrjybphUYY4SV3WT86idP1fYc5ImfqZpdpov5Xv-n22oOLxSIDd9UuhvVRE=',
    }

    headers = {
        'authority': 'www.tiktok.com',
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'en-US,en;q=0.9',
        'cache-control': 'max-age=0',
        'sec-ch-ua': '"Not_A Brand";v="99", "Chromium";v="99", "Google Chrome";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'none',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.9999.999 Safari/537.36',
    }

    # A helper function to extract the date of a tiktok post
    def get_date(tiktok_url):
        # Extract the video ID from the TikTok URL by splitting on "/video/" and "?".
        # Implementation remains the same

    # Scrape stats per video, currently set to the first 100
    def get_video_stats(videos_list):
        # Implementation remains the same

    # Read in the output.json file containing profiles
    with open('output.json', 'r') as file:
        profile_data = json.load(file)

    # Iterate through the profiles and get the video lists, then output in json file
    for profile in profile_data:
        if profile.get('video_details'):
            continue
        video_details = get_video_stats(profile['videos_list'])
        profile['video_details'] = video_details
        with open('outputfinal.json', 'w') as json_file:
            json.dump(profile_data, json_file)



schedule.every().day.at("07:00").do(phase_1()) 
schedule.every().day.at("07:00").do(phase_2()) 

schedule.every().day.at("19:00").do(phase_1()) 
schedule.every().day.at("19:00").do(phase_2)) 

while True:
    schedule.run_pending()
    time.sleep(60)
