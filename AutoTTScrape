from selenium import webdriver
import time
from scrapy import Selector
import json
import csv
import requests
from datetime import datetime, timezone

import schedule

###
#PHASE 1 HERE
###
from selenium import webdriver
import time
from scrapy import Selector
import json
import csv

def phase_1():
    SCROLL_PAUSE_TIME = 15
    driver = webdriver.Firefox()

    def get_profile_data(url):
        driver.get(url)
        time.sleep(SCROLL_PAUSE_TIME)

        videos_list = None

        str = "const origOpen=XMLHttpRequest.prototype.open;XMLHttpRequest.prototype.open=function(t,e){this.addEventListener(\"load\",function(){var t;4===this.readyState&&isVideoFetch(e)&&(pushVideoIDs(t=JSON.parse(this.responseText)),checkAutoScroller(t))}),origOpen.apply(this,arguments)};const autoScroller=setInterval(function(){window.scrollTo(0,document.body.scrollHeight)},1e3);function isVideoFetch(t){return /\\/api\\/post\\/item_list\\//.test(t)}function pushVideoIDs(t){t.itemList.forEach(t=>{})}function checkAutoScroller(t){t.hasMore||clearInterval(autoScroller)}"

        time.sleep(5)
        driver.execute_script(str)
        time.sleep(SCROLL_PAUSE_TIME)
        response = Selector(text=driver.page_source)

        videos_list = response.css('[data-e2e="user-post-item-desc"] a::attr(href)').getall()
        videos_list = [url for url in videos_list if 'https' in url]
        
        title = response.css('[data-e2e="user-title"] ::text').get()
        sub_title = response.css('[data-e2e="user-subtitle"] ::text').get()
        following = response.css('[data-e2e="following-count"] ::text').get()
        followers = response.css('[data-e2e="followers-count"] ::text').get()
        likes = response.css('[data-e2e="likes-count"] ::text').get()
        user_bio = response.css('[data-e2e="user-bio"] ::text').get()
        verified = "verified" if bool(response.css('[data-e2e="user-title"] svg')) else "not verified"
        link = response.css('span.css-847r2g-SpanLink.eht0fek2 ::text').get()

        return {
            "title": title,
            "url": url,
            "sub_title": sub_title,
            "following": following,
            "verified": verified,
            "followers": followers,
            "likes": likes,
            "user_bio": user_bio,
            "link": link,
            "videos_list": videos_list,
            "total_vidoes": len(videos_list),
        }

    profile_urls = []
    with open('input.csv', 'r', newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            url = row['URLs']
            profile_urls.append(url)

    main_data = []

    for index, profile_url in enumerate(profile_urls):
        profile_data = get_profile_data(profile_url)
        main_data.append(profile_data)
        with open('output.json', 'w') as json_file: json.dump(main_data, json_file)
        print(index)


###
#PHASE 2 HERE
###
import time
from scrapy import Selector
import requests
import json
from datetime import datetime, timezone

def phase_2():
    # Cookies and headers as provided

    def get_date(tiktok_url):
        # Function to extract date from TikTok URL
        vid_id = tiktok_url.split("/video/")[1].split("?")[0]
        as_binary = bin(int(vid_id))
        first_31_char = as_binary[:33]
        timestamp = int(first_31_char, 2)
        date_object = datetime.fromtimestamp(timestamp, timezone.utc)
        human_date_format = date_object.strftime("%a, %d %b %Y %H:%M:%S (UTC)")
        return human_date_format

    def get_video_stats(videos_list):
        # Function to scrape video stats
        rows = []
        for video in videos_list[:100]:
            try:
                response = requests.get(video, cookies=cookies, headers=headers)
                s = Selector(text=response.text)
                user_id = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first('uniqueId":"(.*?)",')
                bookmark_count = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first('"collectCount":"(.*?)"')
                captions = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first(r'(?
                comment_count = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first('commentCount":(.*?),')
                like_count = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first('diggCount"(.*?),')
                play_count = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first('playCount"(.*?),')
                location = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first('"locationCreated":"(.*?)"')
                share_count = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first('shareCount":(.*?),')
                date = get_date(video)
                eligible_tag = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first('"bc_disclosure_tag_ecommerce_us":"(.*?)"')
                patnership_tag = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first('bc_new_disclosure":"(.*?)"')
                playUrl = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first('playUrl":"(.*?)"')
                music_title_pattern = r'"music":\{[^}]*?"title":"(.*?)"'
                music_title = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first(music_title_pattern)
                music_author_pattern = r'"music":\{[^}]*?"authorName":"(.*?)"'
                authorName = s.css('#__UNIVERSAL_DATA_FOR_REHYDRATION__::text').re_first(music_author_pattern)

                row = {
                    "url": video,
                    "name": user_id,
                    "bookmark_count": bookmark_count,
                    "captions": captions,
                    "comment_count": comment_count,
                    "like_count": like_count,
                    "play_count": play_count,
                    "location": location,
                    "share_count": share_count,
                    "date": date,
                    "eligible_tag": eligible_tag,
                    "patnership_tag": patnership_tag,
                    "playUrl": playUrl,
                    "music_title": music_title,
                    "authorName": authorName
                }
                with open('log.json', 'w') as json_file:
                    json.dump(row, json_file)
                print('#####################')
                print(row)
                print('#####################')
                rows.append(row)
                time.sleep(8)
            except:
                print("):")
                pass
        return rows

    with open('output.json', 'r') as file:
        profile_data = json.load(file)

    for profile in profile_data:
        if profile.get('video_details'):
            continue
        video_details = get_video_stats(profile['videos_list'])
        profile['video_details'] = video_details
        with open('outputfinal.json', 'w') as json_file:
            json.dump(profile_data, json_file)


schedule.every().day.at("07:00").do(phase_1) 
schedule.every().day.at("07:00").do(phase_2) 

schedule.every().day.at("19:00").do(phase_1) 
schedule.every().day.at("19:00").do(phase_2) 

while True:
    schedule.run_pending()
    time.sleep(60)
